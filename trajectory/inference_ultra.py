import os
import sys
import torch
from datetime import datetime
import shutil
import argparse
import cv2

# Add TrajectoryCrafter to Python path if not already there
TRAJ_CRAFTER_PATH = '/root/TrajectoryCrafter'
if TRAJ_CRAFTER_PATH not in sys.path:
    sys.path.append(TRAJ_CRAFTER_PATH)

from demo import TrajCrafter


class CameraMove:
    PAN_LEFT = "0; 0; 0; -2; 0"
    PAN_RIGHT = "0; 0; 0; 2; 0"
    PAN_UP = "0; 0; 0; 0; 2"
    PAN_DOWN = "0; 0; 0; 0; -2"
    ORBIT_LEFT = "0; -30; 0; 0; 0"
    ORBIT_RIGHT = "0; 30; 0; 0; 0"
    ORBIT_UP = "30; 0; 0; 0; 0"
    ORBIT_DOWN = "-20; 0; 0; 0; 0"
    ZOOM_IN = "0; 0; 0.5; 0; 0"
    ZOOM_OUT = "0; 0; -0.5; 0; 0"

def run_trajectory_crafter(
    video_path: str,
    camera_move: str,  # Use CameraMove constants
    stride: int = 1,  # 1-3
    center_scale: float = 1.0,  # 0.1-2.0
    sampling_steps: int = 50,  # 1-50
    random_seed: int = 43,  # 0-2^31
    output_dir: str = None,
    device: str = "cuda:0",
    mode: str = "gradual"  # gradual, direct, or bullet
) -> str:
    """
    Run TrajectoryCrafter on a video with specified parameters.
    
    Args:
        video_path: Path to input video
        camera_move: Camera movement parameters (use CameraMove constants)
        stride: Frame sampling stride (1-3)
        center_scale: Center scale factor (0.1-2.0)
        sampling_steps: Number of diffusion steps (1-50)
        random_seed: Random seed (0-2^31)
        output_dir: Output directory (default: ./experiments/timestamp)
        device: Device to run on (default: cuda:0)
        mode: Camera mode (gradual, direct, or bullet)
        
    Returns:
        Path to generated video file
    """
    # Setup output directory
    if output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        output_dir = f'./experiments/{timestamp}'
    os.makedirs(output_dir, exist_ok=True)

    # Configure options
    opts = type('Opts', (), {})()
    opts.video_path = video_path
    opts.stride = stride
    opts.radius_scale = center_scale
    opts.diffusion_inference_steps = sampling_steps
    opts.seed = random_seed
    opts.save_dir = output_dir
    opts.device = device
    opts.mode = mode
    opts.weight_dtype = torch.bfloat16
    opts.video_length = 49
    opts.fps = 20
    opts.mask = False
    opts.camera = 'target'
    opts.target_pose = [float(x) for x in camera_move.split(';')]
    
    # Add prompts
    opts.refine_prompt = ". The video is of high quality, and the view is very clear. High quality, masterpiece, best quality, highres, ultra-detailed, fantastic."
    opts.negative_prompt = "The video is not of a high quality, it has a low resolution. Watermark present in each frame. The background is solid. Strange body and strange trajectory. Distortion."
    
    # Model paths
    opts.model_name = 'alibaba-pai/CogVideoX-Fun-V1.1-5b-InP'
    opts.transformer_path = "TrajectoryCrafter/TrajectoryCrafter"
    opts.unet_path = "tencent/DepthCrafter"
    opts.pre_train_path = "stabilityai/stable-video-diffusion-img2vid-xt"
    opts.blip_path = "Salesforce/blip2-opt-2.7b"
    
    # Additional parameters from gradio_app.py
    # Get input video dimensions
    # FIXME: hard coded
    cap = cv2.VideoCapture(video_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cap.release()
    # FIXME: hard coded
    opts.sample_size = [528, 720]
    opts.diffusion_guidance_scale = 6.0
    opts.sampler_name = "DDIM_Origin"
    opts.low_gpu_memory_mode = False
    opts.cpu_offload = 'model'
    opts.depth_inference_steps = 5
    opts.depth_guidance_scale = 1.0
    opts.window_size = 110
    opts.overlap = 25
    opts.max_res = 1024
    
    # Missing parameters found in gradio_app.py and demo.py
    opts.near = 0.0001  # From inference.py defaults
    opts.far = 10000.0  # From inference.py defaults
    opts.anchor_idx = 0  # From inference.py defaults
    opts.cut = 20  # Used in direct mode
    opts.traj_txt = None  # Used for 'traj' camera mode
    
    # Additional parameters for the pipeline
    opts.prompt = None  # Will be auto-generated by get_caption
    opts.height = opts.sample_size[0]
    opts.width = opts.sample_size[1]
    
    # Initialize and run TrajectoryCrafter
    crafter = TrajCrafter(opts)
    
    if mode == 'gradual':
        crafter.infer_gradual(opts)
    elif mode == 'direct':
        crafter.infer_direct(opts)
    elif mode == 'bullet':
        crafter.infer_bullet(opts)
        
    return os.path.join(output_dir, 'gen_reversed.mp4')

def run_trajectory_crafter_with_save(
    video_path: str,
    camera_move: str,  # Use CameraMove constants
    save_path: str = None,  # New parameter for custom save location
    stride: int = 1,  # 1-3
    center_scale: float = 1.0,  # 0.1-2.0
    sampling_steps: int = 50,  # 1-50
    random_seed: int = 43,  # 0-2^31
    output_dir: str = None,
    device: str = "cuda:0",
    mode: str = "gradual"  # gradual, direct, or bullet
) -> str:
    """
    Run TrajectoryCrafter and save the output to a custom location.
    
    Args:
        video_path: Path to input video
        camera_move: Camera movement parameters (use CameraMove constants)
        save_path: Custom path where to save the final video (optional)
        stride: Frame sampling stride (1-3)
        center_scale: Center scale factor (0.1-2.0)
        sampling_steps: Number of diffusion steps (1-50)
        random_seed: Random seed (0-2^31)
        output_dir: Output directory (default: ./experiments/timestamp)
        device: Device to run on (default: cuda:0)
        mode: Camera mode (gradual, direct, or bullet)
        
    Returns:
        Path to the final saved video file
    """
    # Run the original function
    temp_output_path = run_trajectory_crafter(
        video_path=video_path,
        camera_move=camera_move,
        stride=stride,
        center_scale=center_scale,
        sampling_steps=sampling_steps,
        random_seed=random_seed,
        output_dir=output_dir,
        device=device,
        mode=mode
    )
    
    # If save_path is provided, move the file to the desired location
    if save_path:
        # Create the directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
        # Copy the file to the new location
        shutil.copy2(temp_output_path, save_path)
        return save_path
    
    return temp_output_path

if __name__ == "__main__":
    # Add argument parsing
    parser = argparse.ArgumentParser(description='Run TrajectoryCrafter with command line arguments')
    parser.add_argument('--video_path', type=str, required=True, help='Path to input video')
    parser.add_argument('--camera_move', type=str, default=CameraMove.ZOOM_IN, help='Camera movement parameters')
    parser.add_argument('--save_path', type=str, help='Path to save the output video')
    parser.add_argument('--stride', type=int, default=2, help='Frame sampling stride (1-3)')
    parser.add_argument('--center_scale', type=float, default=1.0, help='Center scale factor (0.1-2.0)')
    parser.add_argument('--sampling_steps', type=int, default=50, help='Number of diffusion steps (1-50)')
    parser.add_argument('--random_seed', type=int, default=43, help='Random seed (0-2^31)')
    parser.add_argument('--mode', type=str, default='gradual', help='Camera mode (gradual, direct, or bullet)')
    
    args = parser.parse_args()
    
    output_path = run_trajectory_crafter_with_save(
        video_path=args.video_path,
        camera_move=args.camera_move,
        save_path=args.save_path,
        stride=args.stride,
        center_scale=args.center_scale,
        sampling_steps=args.sampling_steps,
        random_seed=args.random_seed,
        mode=args.mode
    )
    print(f"Generated video saved to: {output_path}")